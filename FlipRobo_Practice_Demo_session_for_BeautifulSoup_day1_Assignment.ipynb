{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOm6x1u+ETKufEMwKNnG1sN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nallagondu/Data-assisgnment/blob/main/FlipRobo_Practice_Demo_session_for_BeautifulSoup_day1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB SCRAPING\n",
        "In all the following questions, you have to use BeautifulSoup to scrape different websites and\n",
        "collect data as per the requirement of the question.\n",
        "Every answer to the question should be in form of a python function which should take URL as the\n",
        "parameter. Use Jupyter Notebooks to program, upload it on your GitHub and send the link of the\n",
        "\n",
        "\n",
        "**1) Write a python program to display all the header tags from wikipedia.org.**"
      ],
      "metadata": {
        "id": "9rjyBQ75sG-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MB68z7XmB5jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lH73HmLBMktx"
      },
      "outputs": [],
      "source": [
        "!pip install bs4\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import Beautifulsoup  libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "qwPT1L8WW4hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_page_responce  = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
        "wiki_page_responce"
      ],
      "metadata": {
        "id": "Ap5JcMpFXSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if wiki_page_responce.status_code == 200:\n",
        "  soup = BeautifulSoup(wiki_page_responce.content)\n",
        "  print(\"can retrive the data\")\n",
        "else:\n",
        "  print(\"failed to retrive the data form wiki\")"
      ],
      "metadata": {
        "id": "Qy-KuOINfS88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup"
      ],
      "metadata": {
        "id": "uA-Kq9KTX3B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scraping  names\n",
        "\n",
        "Wiki_header = soup.find('span',class_=\"mw-headline\")\n",
        "#Wiki_header = wiki_soup.find('span', class_= \"other-project-title\")\n"
      ],
      "metadata": {
        "id": "f-2PIFN6YSi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wiki_header.text"
      ],
      "metadata": {
        "id": "H7AVVd3Oph_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Wiki_header = [] # Empty list  for store the data\n",
        "for i in soup.find_all('span',class_=\"mw-headline\"):\n",
        "   Wiki_header.append(i.text)\n",
        "\n",
        "Wiki_header"
      ],
      "metadata": {
        "id": "SyrFora0w6xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a data frame\n",
        "import pandas as pd\n",
        "wiki_df = pd.DataFrame({'Wiki headings': Wiki_header})\n",
        "wiki_df"
      ],
      "metadata": {
        "id": "yTYIeYh8xg3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year\n",
        "of release) and make data frame"
      ],
      "metadata": {
        "id": "webfD6AAr94H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "ekLGpOLpsT0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_page = requests.get('https://www.imdb.com/list/ls091520106/')\n",
        "imdb_page"
      ],
      "metadata": {
        "id": "bjotwRGUsbAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_soup = BeautifulSoup(imdb_page.content)\n",
        "imdb_soup"
      ],
      "metadata": {
        "id": "Dn0iglC9tOf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping the Name\n",
        "title_of_the_movie = imdb_soup.find('h3', class_=\"lister-item-header\")\n",
        "Movie_title = title_of_the_movie.text.replace('\\n', ' ')\n",
        "Movie_title"
      ],
      "metadata": {
        "id": "ARvywWPFthjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating =  imdb_soup.find('div', class_=\"ipl-rating-widget\")\n",
        "rating.text.replace('\\n','').replace('Rate', '').replace('Error: please try again.','').replace('123456789100','')"
      ],
      "metadata": {
        "id": "BV6QNy_-0Dqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year =  imdb_soup.find('span', class_=\"lister-item-year text-muted unbold\")\n",
        "year.text.replace('(',' ').replace(')', '')"
      ],
      "metadata": {
        "id": "9eBFhQLW1RCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scraping the full data :**"
      ],
      "metadata": {
        "id": "NrqPh7Us5RXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles_of_the_movie = []\n",
        "for i in imdb_soup.find_all('h3', class_=\"lister-item-header\"):\n",
        "  titles_of_the_movie.append(i.text.replace('\\n',''))\n",
        "\n",
        "titles_of_the_movie"
      ],
      "metadata": {
        "id": "ixWBaee21qbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_of_the_movies = []\n",
        "for i in imdb_soup.find_all('div', class_=\"ipl-rating-widget\"):\n",
        "  rating_of_the_movies.append(i.text.replace('\\n','').replace('Rate', '').replace('Error: please try again.','').replace('123456789100',''))\n",
        "\n",
        "rating_of_the_movies"
      ],
      "metadata": {
        "id": "6aiJiCfH6RBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_of_the_movie = []\n",
        "for i in imdb_soup.find_all('span', class_=\"lister-item-year text-muted unbold\"):\n",
        "  year_of_the_movie.append(i.text.replace('(',' ').replace(')', ''))\n",
        "\n",
        "year_of_the_movie"
      ],
      "metadata": {
        "id": "8B0KXdSo7K0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(titles_of_the_movie),len(rating_of_the_movies),len(year_of_the_movie))"
      ],
      "metadata": {
        "id": "JMk0e5zJ7gsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make dataframe\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({'Movie Titles': titles_of_the_movie, 'Movie rating':rating_of_the_movies,'Movie Year':year_of_the_movie})"
      ],
      "metadata": {
        "id": "s5FLwF6r-9I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df\n"
      ],
      "metadata": {
        "id": "jnn49cwn_Nzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Write a python program to scrape mentioned details from dineout.co.in :**\n",
        " i) Restaurant name ii) Cuisine iii) Location iv) Ratings v) Image URL."
      ],
      "metadata": {
        "id": "osIsppraKs4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the required libries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "FpO2RqwvLExm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://www.dineout.co.in/bangalore-restaurants?loc=Bangalore')\n",
        "page"
      ],
      "metadata": {
        "id": "JXVXc2jgLIYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(page.content)\n",
        "soup\n"
      ],
      "metadata": {
        "id": "UZH-uHuFLXPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First we will use html tag where we have the first title of the restarents\n",
        "\n",
        "Restaurant_name = soup.find('div',class_=\"restnt-info cursor\")\n",
        "Name = Restaurant_name.text.split(',')[0].strip()\n",
        "Name"
      ],
      "metadata": {
        "id": "xtekSQbDLbTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scraping first location\n",
        "\n",
        "\n",
        "Restaurant_location = soup.find('div', class_=\"restnt-loc ellipsis\")\n",
        "\n",
        "location=Restaurant_location.text.split(',')[1].strip()\n",
        "location"
      ],
      "metadata": {
        "id": "IBeEmeBCOHG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scraping Cuisine\n",
        "\n",
        "Cuisine = soup.find('span', class_=\"double-line-ellipsis\")\n",
        "Cuisine_details  = Cuisine.text.split('|')[1].strip()\n",
        "#sp1= location.split('|')\n",
        "Cuisine_details\n",
        "\n"
      ],
      "metadata": {
        "id": "hr9DB5YCOSnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restaurant_rating = soup.find('div', class_=\"restnt-rating rating-4\")\n",
        "rating = Restaurant_rating.text\n",
        "rating"
      ],
      "metadata": {
        "id": "VFet751-QUul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "price_element = soup.find('span', class_='double-line-ellipsis')\n",
        "price = price_element.text.replace('₹','').split('|')[0].strip() # removed the rupee symble using repalce\n",
        "price"
      ],
      "metadata": {
        "id": "LLeEcrt4VHCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = [] # Empty list  for store the data\n",
        "for i in soup.find_all('img',class_=\"no-img\"):\n",
        "    images.append(i['data-src'])\n",
        "\n",
        "images"
      ],
      "metadata": {
        "id": "LJufxr1bVbUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restarent_names = [] # Empty list  for store the data\n",
        "for i in soup.find_all('div',class_=\"restnt-info cursor\"):\n",
        "   Restarent_names.append(i.text.split(',')[0].strip())\n",
        "Restarent_names\n"
      ],
      "metadata": {
        "id": "JAp70HI4XPPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restarents_location = [] # Empty list  for store the data\n",
        "for i in soup.find_all('div', class_=\"restnt-loc ellipsis\"):\n",
        "   Restarents_location.append(i.text.split(',')[1].strip())\n",
        "Restarents_location"
      ],
      "metadata": {
        "id": "yxMzpSPNXz0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restarents_Cuisine = [] # Empty list  for store the data\n",
        "for i in soup.find_all('span', class_=\"double-line-ellipsis\"):\n",
        "   Restarents_Cuisine.append(i.text.split('|')[1].strip())\n",
        "Restarents_Cuisine\n"
      ],
      "metadata": {
        "id": "eXMiMDqLYZBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Restarents_rating = [] # Empty list  for store the data\n",
        "for i in soup.find_all('div', class_=\"restnt-rating rating-4\"):\n",
        "   Restarents_rating.append(i.text)\n",
        "Restarents_rating\n"
      ],
      "metadata": {
        "id": "imIFBGT9Y8LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restarent_price = []\n",
        "for i in soup.find_all('span', class_='double-line-ellipsis'):\n",
        "  restarent_price.append(i.text.replace('₹','').split('|')[0].strip()) # removed the rupee symble using repalce\n",
        "restarent_price"
      ],
      "metadata": {
        "id": "8XnznsvkZWqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(Restarent_names),len(Restarents_location),len(Restarents_Cuisine),len(Restarents_rating),len(restarent_price),len(images))"
      ],
      "metadata": {
        "id": "P7dXyhJjVpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rating is not coming for all restarents and it is not uploading the data in Dataframe .So rating is removed for now"
      ],
      "metadata": {
        "id": "cdhr_SJ8eybn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'Restarent_names':Restarent_names,'Restarents_location':Restarents_location,'Restarents_Cuisine':Restarents_Cuisine,'restarent_price':restarent_price,'Images':images})"
      ],
      "metadata": {
        "id": "fU1LOmMkdRIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "3dv0nhHYebsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4) Write s python program to display list of respected former President's  of India(i.e.Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm and make data frame\n",
        "\n",
        "Note: as per give URl we only can see Presiedent details ,So I worked for the same not for **finance ministors** data"
      ],
      "metadata": {
        "id": "guceOySafDOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import Beautifulsoup  libraries\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "IH8TuVSSB7Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get('https://presidentofindia.nic.in/former-presidents')\n",
        "page"
      ],
      "metadata": {
        "id": "rEzuXkbOOqjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMI_soup = BeautifulSoup(page.content)\n",
        "PMI_soup"
      ],
      "metadata": {
        "id": "PDNVZQcQO87t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to get Names of farmer presidents of Bharath\n",
        "pofB_name = PMI_soup.find('div',class_=\"desc-sec\")\n",
        "pofB_name.h3.text.strip()\n"
      ],
      "metadata": {
        "id": "Ibr9bjpLPfoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to get Tern of farmer presidents of Bharath\n",
        "pofB_Term  = PMI_soup.find('div',class_=\"desc-sec\")\n",
        "pofB_Term.h5.text.strip()"
      ],
      "metadata": {
        "id": "TDY0Ie3vSVXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pofB_names = [] # Empty list  for store the data\n",
        "for i in PMI_soup.find_all('div',class_=\"desc-sec\"):\n",
        "  pofB_names.append(i.h3.text.strip())\n",
        "pofB_names"
      ],
      "metadata": {
        "id": "29osBs1TY1dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pofB_term = [] # Empty list  for store the data\n",
        "for i in PMI_soup.find_all('div',class_=\"desc-sec\"):\n",
        "  pofB_term.append(i.h5.text.strip())\n",
        "pofB_term"
      ],
      "metadata": {
        "id": "5ZsdBl--ZrEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "\n",
        "for i in PMI_soup.find_all('div',class_=\"img-sec\"):\n",
        "    img_tag = i.find('img')\n",
        "    if img_tag:\n",
        "      src = img_tag.get('src')\n",
        "      images.append(src)\n",
        "images"
      ],
      "metadata": {
        "id": "FO9cb5JJaVQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pofB_names),len(pofB_term),len(images))"
      ],
      "metadata": {
        "id": "f0-GekEvc6Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "PRB_df = pd.DataFrame({'Prisedent_names':pofB_names,'Prisedent_term':pofB_term,'Images':images})\n",
        "\n",
        "PRB_df"
      ],
      "metadata": {
        "id": "VB3WNhweZ7Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRB_df.sort_values(by = 'Prisedent_term')"
      ],
      "metadata": {
        "id": "SF3aVppvdq7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}